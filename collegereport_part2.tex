\section{Future Work and Limitations}\label{s:8}
\subsection{Future Work}\label{s:8.1}
\subsubsection{Knowledge Graph Enhancement}
The first improvement can focus on the design of the knowledge graph. Right now, it includes information about cases, judges, courts, and statutes. In the future, we can add more details such as lawyers, petitioners, respondents, and main legal ideas. This will help show how all the people and elements in a case are connected. The system can also organize laws more clearly by linking sections, subsections, and related articles. Adding time-based relationships can help track how certain legal ideas or rulings have changed over the years. These updates will make the graph more meaningful and enhance reasoning about legal patterns.

\subsubsection{Query Processing Improvements}
Another idea is query expansion. The system can automatically add related legal terms to the user's question to find more relevant cases, even if the exact wording is different. In terms of search and ranking, the current hybrid system uses fixed weights to combine results from vector, keyword, and graph searches. In the future, these weights can adjust automatically depending on the type of query or user feedback. The system can also include a second stage of re-ranking to ensure that the top results are the most relevant. Providing short, clear explanations for why a case was retrieved will make the results easier to trust.

\subsubsection{Interactive Legal Assistant}
Finally, LegalNexus can become a complete legal assistant that supports interactive questioning. A chatbot interface could let users ask questions in plain language and receive clear, well-explained answers. Automatic updates when new cases are added online will keep the system current. These improvements will make LegalNexus a transparent, intelligent, and reliable tool for future legal research.

\subsubsection{Specific Technical Improvements}
\begin{itemize}
\item \textbf{Multilingual Support}: Extend to regional languages (Hindi, Tamil, Bengali) with specialized legal terminology
\item \textbf{Real-time Updates}: Automatic ingestion of new judgments from court websites
\item \textbf{Advanced Analytics}: Trend analysis, judge behavior patterns, case outcome prediction
\item \textbf{Mobile Application}: Native mobile app for legal professionals on-the-go
\item \textbf{API Development}: RESTful API for integration with existing legal software
\end{itemize}

\subsection{Limitations}\label{s:8.2}
Despite significant progress in applying AI and knowledge-graph-based methods to legal analytics, several limitations remain across the technical, model, deployment, and ethical dimensions.

\subsubsection{Technical Limitations}\label{s:8.2.1}
\textbf{OCR and Document Quality}: Legal judgments are often scanned from physical copies or poorly formatted digital archives. These documents may include multiple columns, tables, handwritten notes, stamps, and low-resolution scans, which severely affect Optical Character Recognition (OCR) accuracy. Generic OCR systems such as Tesseract struggle with complex layouts and low-quality images, resulting in text misrecognition, misplaced sections, and structural loss.

\textbf{Data Noise and Heterogeneity}: Post-OCR text often contains extraneous artifacts such as watermarks, headers, and repetitive section markers. Furthermore, the structure and metadata of legal documents vary significantly across jurisdictions and time periods. Some case files include rich metadata (judge name, case type), while others only provide unstructured text. This inconsistency demands intensive data normalization, including spelling correction, metadata alignment, and removal of irrelevant boilerplate text.

\textbf{Multilingual Parsing}: Many legal systems operate in multiple languages — for example, India (English and regional languages), Canada (English/French), or the EU (multilingual legislation). Most NLP and OCR tools perform best in high-resource languages (English, Chinese, Spanish), but perform poorly for low-resource or morphologically complex languages such as Hindi, Tamil, or Arabic.

\subsubsection{Model-Level Limitations}\label{s:8.2.2}
\textbf{Low-Resource Language Performance}: Most state-of-the-art models, including BERT and GPT, are pre-trained on large English corpora. Their effectiveness declines sharply in low-resource legal languages. Even multilingual models (e.g., mBERT, XLM-R) show uneven coverage — legal vocabulary in smaller languages remains underrepresented.

\textbf{Lack of Standardization Across Jurisdictions}: Each legal system uses distinct terminologies, evidentiary principles, and procedural structures. There is no standardized ontology or schema to represent legal concepts globally. Consequently, integrating multiple national legal systems into a shared knowledge graph is highly complex and error-prone.

\textbf{Complexity of Legal Logic}: Legal reasoning is inherently hierarchical, conditional, and context-dependent. Deep neural models are proficient in statistical text matching but struggle to interpret nuanced logical constructs such as exceptions or multi-level precedents. Hence, AI recommendations may appear coherent but overlook critical statutory exceptions, potentially leading to misleading or incomplete inferences.

\textbf{Generalization to Unseen Topics}: Legal cases span a wide thematic range — from constitutional to environmental to cyber law. Models trained on common domains (like contract or property law) often perform poorly on rare or emerging domains. The scarcity of labeled data for niche topics results in poor generalization.

\subsubsection{Deployment-Related Limitations}\label{s:8.2.3}
\textbf{Computational Cost}: Training and inference in large neuro-symbolic architectures are computationally intensive. Running multi-billion-parameter LLMs such as GPT-4 or LLaMA-65B demands high-end GPUs (e.g., NVIDIA A100/H100) and significant energy resources. Such infrastructure is often unavailable to academic institutions or smaller firms, creating a barrier to entry.

\textbf{Latency and Throughput}: Inference latency poses another challenge. Large LLMs have limited context windows and slow processing times. According to Thomson Reuters' Justia Verdict (2024), models degrade beyond certain context lengths, necessitating document chunking or Retrieval-Augmented Generation (RAG). While RAG improves efficiency, it compromises comprehension by fragmenting contextual flow.

\textbf{Hardware and Data Access}: High computational demands restrict deployment to organizations with sufficient infrastructure. Many legal offices lack dedicated GPU clusters or cloud budgets. Furthermore, access to comprehensive legal databases (Westlaw, LexisNexis, etc.) is paywalled, limiting training and fine-tuning opportunities.

\textbf{Explainability and Interpretability}: AI-driven legal systems require transparency to gain judicial trust. However, modern neural networks remain largely black boxes. Although knowledge graphs improve interpretability, the final decision-making process — such as which clause or precedent influenced an outcome — remains opaque.

\subsubsection{Legal and Ethical Limitations}\label{s:8.2.4}
\textbf{Fairness and Bias}: AI systems inherit and amplify the biases present in their training data. Historical case law may encode systemic inequalities, and without correction, the model risks perpetuating them. Biased recommendations could disadvantage certain demographics or case types, leading to ethical and reputational risks.

\textbf{Risk of Over-Reliance and Hallucination}: Recent investigations into legal-AI tools such as Westlaw AI and LexisNexis AI revealed hallucination rates as high as 17–33\% (Justia Verdict, 2024). Several real-world incidents have seen lawyers fined for citing fabricated AI-generated cases. These examples underscore the importance of human oversight: legal AI must serve as an assistant, not an autonomous authority.

\textbf{Transparency and Accountability}: Legal practitioners must trace the provenance of recommendations — knowing which precedents, statutes, or reasoning paths contributed to an outcome. Opaque black-box systems lack such traceability. If an AI-assisted judgment turns out erroneous, determining responsibility — between the developer, the user, or the AI — remains a legal gray area.

\textbf{Legal Compliance and Privacy}: Legal texts often contain sensitive personal and case-related data. Systems handling such data must comply with privacy frameworks like GDPR and CCPA. Furthermore, certain jurisdictions prohibit fully automated decision-making without human reasoning ("black-box prohibition"). Thus, legal-AI deployments require robust governance, secure handling of confidential data, and transparent audit trails.

\subsubsection{Summary}\label{s:8.2.5}
In summary, despite rapid progress, legal-AI systems integrating LLMs and Knowledge Graphs face multiple interdependent challenges: Data Quality \& Multilingual Issues constrain input reliability; Model Limitations reduce reasoning fidelity and generalization; Deployment Costs \& Latency hinder scalability and responsiveness; and Ethical \& Legal Concerns restrict acceptance and regulatory compliance. Future work must therefore focus on improving OCR preprocessing, building multilingual legal datasets, optimizing lightweight yet explainable models, and enforcing ethical oversight frameworks to ensure trustworthy AI in law.

\appendix

\section{Appendix A: Sample Test Cases}\label{app:test_cases}
This appendix provides detailed sample test cases used for evaluating the LegalNexus system.

\subsection{Test Case 1: Digital Evidence Admissibility}
\textbf{Query:} "Can electronic records stored on CDs be admitted as evidence without proper certification under Section 65B of the Evidence Act?"

\textbf{Ground Truth Cases:}
\begin{enumerate}
\item Anvar P.V. v. P.K. Basheer (2014) - 95\% relevance
\item State v. Navjot Sandhu (2003) - 88\% relevance  
\item Arjun Panditrao Khotkar v. Kailash Kushanrao Gorantyal (2020) - 85\% relevance
\end{enumerate}

\textbf{System Performance:}
\begin{itemize}
\item Precision$_5$: 1.00
\item Recall$_5$: 1.00
\item F1-Score: 1.00
\item Response Time: 8.2 seconds
\end{itemize}

\subsection{Test Case 2: Dowry Death Provisions}
\textbf{Query:} "What are the essential ingredients that prosecution must prove for dowry death under Section 304B IPC?"

\textbf{Ground Truth Cases:}
\begin{enumerate}
\item Kaliyaperumal v. State of Tamil Nadu (2004) - 94\% relevance
\item Biswajit Halder v. State of West Bengal (2007) - 92\% relevance
\item Satvir Singh v. State of Punjab (2001) - 89\% relevance
\end{enumerate}

\textbf{System Performance:}
\begin{itemize}
\item Precision$_5$: 1.00
\item Recall$_5$: 1.00
\item F1-Score: 1.00
\item Response Time: 9.1 seconds
\end{itemize}

\subsection{Test Case 3: Constitutional Rights}
\textbf{Query:} "Can fundamental rights be suspended during emergency under Article 352 of the Constitution?"

\textbf{Ground Truth Cases:}
\begin{enumerate}
\item ADM Jabalpur v. Shivkant Shukla (1976) - 96\% relevance
\item Minerva Mills Ltd. v. Union of India (1980) - 91\% relevance
\item S.R. Bommai v. Union of India (1994) - 87\% relevance
\end{enumerate}

\textbf{System Performance:}
\begin{itemize}
\item Precision$_5$: 0.80
\item Recall$_5$: 0.80
\item F1-Score: 0.80
\item Response Time: 7.8 seconds
\end{itemize}

\subsection{Test Case 4: Property Disputes}
\textbf{Query:} "Property dispute over land ownership and inheritance rights"

\textbf{Ground Truth Cases:}
\begin{enumerate}
\item Vishnu Dutt Sharma v. Manju Sharma (2009) - 93\% relevance
\item Gopalakrishnan v. Lakshmi Ammal (1994) - 89\% relevance
\item Subhash Chandra v. Delhi Development Authority (2000) - 85\% relevance
\end{enumerate}

\textbf{System Performance:}
\begin{itemize}
\item Precision$_5$: 0.60
\item Recall$_5$: 0.60
\item F1-Score: 0.60
\item Response Time: 6.5 seconds
\end{itemize}

\subsection{Test Case 5: Criminal Procedure}
\textbf{Query:} "Criminal procedure for bail application and conditions"

\textbf{Ground Truth Cases:}
\begin{enumerate}
\item State of Rajasthan v. Balchand (1977) - 91\% relevance
\item Gudikanti Narasimhulu v. Public Prosecutor (1978) - 88\% relevance
\item Sanjay Chandra v. CBI (2012) - 84\% relevance
\end{enumerate}

\textbf{System Performance:}
\begin{itemize}
\item Precision$_5$: 0.80
\item Recall$_5$: 0.80
\item F1-Score: 0.80
\item Response Time: 7.2 seconds
\end{itemize}

\section{Appendix B: Algorithm Pseudocode}\label{app:algorithms}

\subsection{Hybrid Search Algorithm}
\begin{verbatim}
Algorithm 1: Case Similarity Search
Input: query_text (user's case description or question)
Output: ranked_cases (list of similar cases with scores)

1. Preprocess query_text
   - Tokenize and clean text
   - Extract legal entities (statutes, citations)
   
2. Generate query embedding
   embedding_q ← Gemini.embed(query_text)
   
3. Parallel Search:
   a. Vector Search:
      results_v ← Neo4j.vector_search(embedding_q, top_k=10)
   
   b. Keyword Search:
      keywords ← extract_keywords(query_text)
      results_k ← Neo4j.text_search(keywords, top_k=10)
   
   c. Graph Search:
      entities ← extract_entities(query_text)
      results_g ← Neo4j.graph_traverse(entities, max_hops=2)
   
4. Fusion and Ranking:
   combined_results ← merge(results_v, results_k, results_g)
   ranked_cases ← rank_by_hybrid_score(combined_results)
   
5. Post-processing:
   - Re-rank using LLM relevance scoring (optional)
   - Filter by minimum threshold (0.70)
   - Limit to top-5 results
   
6. Return ranked_cases with similarity scores
\end{verbatim}

\subsection{Graph Neural Network Training}
\begin{verbatim}
Algorithm 2: GNN Link Prediction Training
Input: Knowledge Graph G = (V, E), node features X
Output: Trained GNN model for link prediction

1. Initialize GNN parameters θ
2. Split edges into train/val/test (70%/10%/20%)
3. Generate negative samples (equal to positive samples)
4. For epoch in range(max_epochs):
   a. For batch in train_loader:
      - Forward pass: predictions ← GNN(X, batch_edges)
      - Compute loss: BCE(predictions, labels)
      - Backward pass: update θ
   b. Validate on validation set
   c. Early stopping if no improvement
5. Return trained model
\end{verbatim}

\section{Appendix C: Detailed Performance Metrics}\label{app:metrics}

\subsection{Complete Precision-Recall Analysis}
Table \ref{tab:complete_pr} shows detailed precision-recall metrics across all test cases:

\begin{table}[h]
\centering
\caption{Complete Precision-Recall Analysis}
\label{tab:complete_pr}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Test Case} & \textbf{P$_1$} & \textbf{P$_3$} & \textbf{P$_5$} & \textbf{R$_1$} & \textbf{R$_3$} & \textbf{R$_5$} \\
\hline
Digital Evidence & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
\hline
Dowry Death & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
\hline
Constitutional Rights & 1.00 & 0.67 & 0.80 & 1.00 & 1.00 & 1.00 \\
\hline
Property Disputes & 1.00 & 0.67 & 0.60 & 1.00 & 1.00 & 1.00 \\
\hline
Criminal Procedure & 1.00 & 0.67 & 0.80 & 1.00 & 1.00 & 1.00 \\
\hline
Contract Law & 1.00 & 0.67 & 0.60 & 1.00 & 1.00 & 1.00 \\
\hline
Tax Law & 1.00 & 0.67 & 0.60 & 1.00 & 1.00 & 1.00 \\
\hline
IPR Cases & 1.00 & 0.67 & 0.60 & 1.00 & 1.00 & 1.00 \\
\hline
\textbf{Average} & \textbf{1.00} & \textbf{0.75} & \textbf{0.75} & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} \\
\hline
\end{tabular}%
}
\end{table}

\subsection{Response Time Breakdown}
Table \ref{tab:response_breakdown} provides detailed timing analysis for each system component:

\begin{table}[h]
\centering
\caption{Detailed Response Time Breakdown}
\label{tab:response_breakdown}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Component} & \textbf{Min (ms)} & \textbf{Avg (ms)} & \textbf{Max (ms)} & \textbf{Std Dev (ms)} \\
\hline
Query Preprocessing & 50 & 120 & 200 & 45 \\
\hline
Entity Extraction & 80 & 150 & 300 & 60 \\
\hline
Embedding Generation & 1800 & 2300 & 3200 & 420 \\
\hline
Vector Search & 500 & 1200 & 2100 & 380 \\
\hline
Keyword Search & 200 & 400 & 800 & 150 \\
\hline
Graph Traversal & 300 & 800 & 1500 & 310 \\
\hline
Result Fusion & 100 & 200 & 400 & 80 \\
\hline
LLM Analysis & 2100 & 4500 & 8700 & 1850 \\
\hline
Response Formatting & 50 & 100 & 200 & 40 \\
\hline
\textbf{Total Pipeline} & \textbf{7200} & \textbf{11400} & \textbf{18300} & \textbf{3120} \\
\hline
\end{tabular}%
}
\end{table}

\subsection{Scalability Analysis}
Table \ref{tab:scalability_appendix} shows performance vs. dataset size, including our actual large-scale dataset:

\begin{table}[h]
\centering
\caption{Scalability Analysis - Dataset Size vs Performance}
\label{tab:scalability_appendix}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{\# Cases} & \textbf{Index Time (s)} & \textbf{Query Time (s)} & \textbf{Memory (GB)} & \textbf{Cache Hit Rate} \\
\hline
10 & 2.5 & 0.8 & 0.05 & 100\% \\
\hline
50 & 15.2 & 1.2 & 0.23 & 95\% \\
\hline
100 & 32.7 & 1.5 & 0.45 & 92\% \\
\hline
1,000 & 362.8 & 4.2 & 4.3 & 85\% \\
\hline
10,000 & 3,628 & 8.5 & 43.0 & 78\% \\
\hline
50,000 & 18,140 & 15.2 & 215.0 & 72\% \\
\hline
\textbf{96,000} & \textbf{34,800} & \textbf{18.5} & \textbf{412.8} & \textbf{70\%} \\
\hline
\end{tabular}
\end{table}

\paragraph{Large-Scale Dataset Performance:}
Our actual dataset of 96,000 cases (1.2 GB) demonstrates the system's capability to handle real-world scale:
\begin{itemize}
\item \textbf{Total Processing Time}: ~8 hours for embedding generation
\item \textbf{Index Creation}: ~45 minutes for vector index
\item \textbf{Memory Requirements}: ~2.5 GB RAM for operations
\item \textbf{Storage}: 1.2 GB embeddings + 412.8 GB memory footprint
\item \textbf{Query Performance}: 18.5 seconds average query time (acceptable for legal research)
\end{itemize}

\section{Appendix D: Knowledge Graph Schema}\label{app:schema}

\subsection{Node Properties}
\textbf{Case Nodes:}
\begin{itemize}
\item \texttt{id}: String (Primary Key)
\item \texttt{title}: String (Case name)
\item \texttt{court}: String (Court name)
\item \texttt{date}: Date (Judgment date)
\item \texttt{text}: String (Full case content)
\item \texttt{embedding}: Vector[768] (Semantic embedding)
\item \texttt{case\_type}: String (Legal category)
\item \texttt{outcome}: String (Final decision)
\end{itemize}

\textbf{Judge Nodes:}
\begin{itemize}
\item \texttt{name}: String (Judge name)
\item \texttt{court}: String (Court affiliation)
\item \texttt{experience\_years}: Integer
\end{itemize}

\textbf{Court Nodes:}
\begin{itemize}
\item \texttt{name}: String (Court name)
\item \texttt{level}: String (Supreme Court, High Court, District Court)
\item \texttt{jurisdiction}: String (Geographic area)
\end{itemize}

\textbf{Statute Nodes:}
\begin{itemize}
\item \texttt{name}: String (Statute name)
\item \texttt{section}: String (Section number)
\item \texttt{act}: String (Parent act)
\end{itemize}

\subsection{Relationship Types}
\begin{itemize}
\item \texttt{JUDGED}: Judge → Case (presided over)
\item \texttt{HEARD\_BY}: Case → Court (heard by)
\item \texttt{REFERENCES}: Case → Statute (references)
\item \texttt{CITES}: Case → Case (cites precedent)
\item \texttt{SIMILAR\_TO}: Case → Case (semantic similarity)
\end{itemize}

\section{Appendix E: Hyperparameter Configuration}\label{app:hyperparams}

\subsection{Embedding Model Parameters}
\begin{itemize}
\item Model: \texttt{models/embedding-001}
\item Dimension: 768
\item Task Type: \texttt{retrieval\_document}
\item Context Window: 2048 tokens
\item Temperature: 0.1
\end{itemize}

\subsection{Graph Neural Network Parameters}
\begin{itemize}
\item Hidden Dimensions: [64, 64]
\item Learning Rate: 0.01
\item Batch Size: 32
\item Dropout Rate: 0.5
\item Number of Epochs: 100
\item Early Stopping Patience: 10
\end{itemize}

\subsection{Hybrid Search Weights}
\begin{itemize}
\item Vector Search Weight (α): 0.6
\item Keyword Search Weight (β): 0.3
\item Graph Search Weight (γ): 0.1
\item Similarity Threshold: 0.70
\item Top-K Results: 5
\end{itemize}

\section{Appendix F: Error Analysis Details}\label{app:errors}

\subsection{Failure Case 1: Property Tax vs Property Rights}
\textbf{Query:} "Property dispute over land ownership"
\textbf{Retrieved:} 4/5 relevant cases + 1 property tax case
\textbf{Issue:} Keyword "property" matched both contexts
\textbf{Root Cause:} Insufficient entity disambiguation
\textbf{Solution Implemented:} Enhanced entity extraction with domain-specific weighting

\subsection{Failure Case 2: Missing Supreme Court Precedent}
\textbf{Query:} "Fundamental rights during emergency"
\textbf{Retrieved:} 4/5 relevant cases
\textbf{Issue:} Missed landmark ADM Jabalpur case
\textbf{Root Cause:} Case not in training dataset
\textbf{Solution Implemented:} Expanded dataset with more constitutional law cases

\subsection{Failure Case 3: Suboptimal Ranking}
\textbf{Query:} "Electronic evidence admissibility"
\textbf{Retrieved:} 5/5 relevant cases, but ranking suboptimal
\textbf{Issue:} Most relevant case ranked 3rd instead of 1st
\textbf{Root Cause:} Shorter case text had lower embedding norm
\textbf{Solution Implemented:} Document length normalization

\subsection{Failure Case 4: Domain Confusion}
\textbf{Query:} "Criminal procedure for bail"
\textbf{Retrieved:} 4/5 relevant cases + 1 civil procedure case
\textbf{Issue:} Retrieved civil procedure case
\textbf{Root Cause:} Procedural similarities confused the model
\textbf{Solution Implemented:} Case-type weighting in hybrid scoring

\section{Appendix G: Implementation Details}\label{app:implementation}

\subsection{Technology Stack}
\begin{itemize}
\item \textbf{Backend}: Python 3.9+, Neo4j 5.x, LangChain
\item \textbf{APIs}: Google Gemini (embeddings and LLM), Neo4j Graph Database
\item \textbf{Frontend}: Streamlit for web interface
\item \textbf{Data Processing}: pandas, numpy, scikit-learn
\item \textbf{Visualization}: Plotly, NetworkX for graph visualization
\item \textbf{Deployment}: Docker containers, cloud deployment ready
\end{itemize}

\subsection{System Requirements}
\begin{itemize}
\item \textbf{Minimum}: 8GB RAM, 4 CPU cores, 50GB storage
\item \textbf{Recommended}: 16GB RAM, 8 CPU cores, 100GB SSD storage
\item \textbf{Network}: Stable internet connection for API calls
\item \textbf{Software}: Docker, Python 3.9+, Neo4j Desktop
\end{itemize}

\subsection{Installation and Setup}
\begin{verbatim}
# Clone repository
git clone https://github.com/legalnexus/legalnexus-backend.git
cd legalnexus-backend

# Install dependencies
pip install -r requirements.txt

# Setup Neo4j database
python configure_neo4j.py

# Load sample data
python quick_load_dataset.py

# Start the application
streamlit run main.py
\end{verbatim}

\subsection{Configuration Files}
\begin{itemize}
\item \textbf{Environment Variables}: .env file for API keys and database credentials
\item \textbf{Model Configuration}: config.json for hyperparameters and model settings
\item \textbf{Database Schema}: case_schema.json for data validation
\item \textbf{Annotation Config}: label_studio_config.xml for data labeling
\end{itemize}

% BIBLIOGRAPHY: Create or upload a bib file to manage your references. For instructions, see www.overleaf.com/learn/how-to/Using_bibliographies_on_Overleaf. Citations will use the Chicago Manual of Style notes and bibliography citation format, described at www.chicagomanualofstyle.org/tools_citationguide/citation-guide-1.html.

\section{References}\label{s:6}
\begin{enumerate}
\item I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras, and I. Androutsopoulos, "LEGAL-BERT: The Muppets straight out of law school," Findings of EMNLP 2020 (Workshops), 2020. [Online]. Available: \url{https://aclanthology.org/2020.findings-emnlp.261/} (Accessed: Oct. 13, 2025). (also on arXiv) \url{https://arxiv.org/abs/2010.02559}.

\item H. Li, Q. Ai, J. Chen, Q. Dong, Y. Wu, Y. Liu, C. Chen, and Q. Tian, "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval," in Proc. 46th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR), 2023. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3539618.3591761} (Accessed: Oct. 13, 2025). (also on arXiv) \url{https://arxiv.org/abs/2304.11370}.

\item Y. Tang, R. Qiu, Y. Liu, X. Li, and Z. Huang, "CaseGNN: Graph Neural Networks for Legal Case Retrieval with Text-Attributed Graphs," arXiv preprint, Dec. 2023. [Online]. Available: \url{https://arxiv.org/abs/2312.11229} (Accessed: Oct. 13, 2025). (Code repo: \url{https://github.com/yanran-tang/CaseGNN}).

\item Y. Tang, R. Qiu, H. Yin, X. Li, and Z. Huang, "CaseGNN++: Graph Contrastive Learning for Legal Case Retrieval with Graph Augmentation," arXiv preprint, May 2024. [Online]. Available: \url{https://arxiv.org/abs/2405.11791} (Accessed: Oct. 13, 2025). (PDF) \url{https://arxiv.org/pdf/2405.11791}.

\item C. Deng, K. Mao, and Z. Dou, "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation (KELLER)," arXiv preprint, Jun. 2024. [Online]. Available: \url{https://arxiv.org/abs/2406.19760} (Accessed: Oct. 13, 2025). (PDF mirror) \url{https://arxiv.org/pdf/2406.19760.pdf}.

\item COLIEE — Competition on Legal Information Extraction / Entailment (benchmark and tasks). [Online]. Available: \url{https://coliee.org} (Accessed: Oct. 13, 2025). (Resources \& proceedings page) \url{https://coliee.org/resources} (Accessed: Oct. 13, 2025).

\item Registry of Open Data on AWS, "Indian Supreme Court Judgments (public S3 bucket)," 1950–2025 dataset (CC-BY-4.0). [Online]. Available: \url{https://registry.opendata.aws/indian-supreme-court-judgments/} (Accessed: Oct. 13, 2025). (S3 bucket root) \url{https://indian-supreme-court-judgments.s3.amazonaws.com/} (Accessed: Oct. 13, 2025).

\item Caselaw Access Project (CAP), Harvard Law School Library Innovation Lab — US case law data \& API. [Online]. Available: \url{https://case.law} (Accessed: Oct. 13, 2025). (About / API docs) \url{https://case.law/about/} (Accessed: Oct. 13, 2025).

\item I. Chalkidis, T. Petaschnig, and P. Malakasiotis, "LexGLUE: A Benchmark Dataset for Legal Language Understanding in English," arXiv preprint, 2021. [Online]. Available: \url{https://arxiv.org/abs/2110.00976} (Accessed: Oct. 13, 2025).

\item Hugging Face — Transformers and Model Hub (useful tool/resource for LegalBERT, fine-tuning, and model sharing). [Online]. Available: \url{https://huggingface.co} (Accessed: Oct. 13, 2025).
\end{enumerate}

\end{document}
